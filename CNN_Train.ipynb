{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12Y1eMYko_3A"
   },
   "source": [
    "# CNN models - model training using transfer learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vtEv2d4oxJb",
    "outputId": "a7952ffe-ddf1-423f-d484-3247024f1deb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:08:01.873006: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-18 09:08:02.081785: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-18 09:08:02.083184: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-18 09:08:02.977238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6PLJ6rfpPo-"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IU7U-I9pb8C",
    "outputId": "a69eae70-298a-4b4a-ef88-4502a41800fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9cf8d97247417e94a04fad607bce6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "# MODEL_CHARACTERISTICS\n",
    "MODELS = {\n",
    "    \"InceptionV1\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/inception-v1/TensorFlow2/feature-vector/2\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "    \"InceptionV2\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/inception-v2/TensorFlow2/feature-vector/2\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "    \"MobileNetV1\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/mobilenet-v1/TensorFlow2/100-224-feature-vector/2\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "    \"MobileNetV2\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/mobilenet-v2/TensorFlow2/100-224-feature-vector/2\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "    \"MobileNetV3\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/mobilenet-v3/TensorFlow2/small-100-224-feature-vector/1\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "    \"NasNetMobile\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/nasnet/TensorFlow2/mobile-feature-vector/2\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "    \"ResNetV1\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/resnet-v1/TensorFlow2/50-feature-vector/2\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "    \"ResNetV2\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/resnet-v2/TensorFlow2/50-feature-vector/2\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "}\n",
    "P_DIM=224\n",
    "NUM_CLASSES=21\n",
    "CT_CFG = widgets.FloatSlider(description=\"Convergenge threshold:\", value=1.0, min=0.25, max=1.00, step=0.01)\n",
    "DROPOUT_CFG = widgets.FloatSlider(description=\"Dropout (0: no dropout):\", value=0.2, min=0.0, max=0.5, step=0.1)\n",
    "MAX_STEPS_CFG = widgets.IntSlider(description=\"Max. training steps:\", value=25, min=1, max=100, step=10)\n",
    "BATCH_SIZE_CFG = widgets.IntSlider(description=\"Batch size:\", value=32, min=32, max=500)\n",
    "display(widgets.VBox([CT_CFG, DROPOUT_CFG, MAX_STEPS_CFG, BATCH_SIZE_CFG]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input dim: 224 \n",
      " Num classes 21 \n",
      " Conv. thr.: 1.0 \n",
      " Dropout: 0.2 \n",
      " Max steps: 25 \n",
      " Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "global P_CT\n",
    "P_CT = CT_CFG.value\n",
    "P_DROPOUT = DROPOUT_CFG.value\n",
    "P_MAX_STEPS = MAX_STEPS_CFG.value\n",
    "P_BATCH_SIZE = BATCH_SIZE_CFG.value\n",
    "print('','Input dim:', P_DIM, '\\n',\n",
    "      'Num classes', NUM_CLASSES, '\\n',\n",
    "      'Conv. thr.:', P_CT, '\\n',\n",
    "      'Dropout:', P_DROPOUT, '\\n',\n",
    "      'Max steps:', P_MAX_STEPS, '\\n',\n",
    "      'Batch size:', P_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>set</th>\n",
       "      <th>time</th>\n",
       "      <th>source</th>\n",
       "      <th>image</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>SS</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>Redmi</td>\n",
       "      <td>video_data/SS/train_Xiaomi_0004_0121.jpeg</td>\n",
       "      <td>../dados_indoor_location/Xiaomi/Vision/1653909...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>SS</td>\n",
       "      <td>train</td>\n",
       "      <td>5</td>\n",
       "      <td>Redmi</td>\n",
       "      <td>video_data/SS/train_Xiaomi_0005_0151.jpeg</td>\n",
       "      <td>../dados_indoor_location/Xiaomi/Vision/1653909...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>SS</td>\n",
       "      <td>train</td>\n",
       "      <td>6</td>\n",
       "      <td>Redmi</td>\n",
       "      <td>video_data/SS/train_Xiaomi_0006_0181.jpeg</td>\n",
       "      <td>../dados_indoor_location/Xiaomi/Vision/1653909...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>SS</td>\n",
       "      <td>train</td>\n",
       "      <td>7</td>\n",
       "      <td>Redmi</td>\n",
       "      <td>video_data/SS/train_Xiaomi_0007_0211.jpeg</td>\n",
       "      <td>../dados_indoor_location/Xiaomi/Vision/1653909...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>SS</td>\n",
       "      <td>train</td>\n",
       "      <td>8</td>\n",
       "      <td>Redmi</td>\n",
       "      <td>video_data/SS/train_Xiaomi_0008_0241.jpeg</td>\n",
       "      <td>../dados_indoor_location/Xiaomi/Vision/1653909...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>AT_I2</td>\n",
       "      <td>train</td>\n",
       "      <td>106</td>\n",
       "      <td>Redmi</td>\n",
       "      <td>video_data/AT_I2/train_Xiaomi_0106_3181.jpeg</td>\n",
       "      <td>../dados_indoor_location/Xiaomi/Vision/1653920...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>AT_I2</td>\n",
       "      <td>train</td>\n",
       "      <td>107</td>\n",
       "      <td>Redmi</td>\n",
       "      <td>video_data/AT_I2/train_Xiaomi_0107_3211.jpeg</td>\n",
       "      <td>../dados_indoor_location/Xiaomi/Vision/1653920...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>AT_I2</td>\n",
       "      <td>train</td>\n",
       "      <td>108</td>\n",
       "      <td>Redmi</td>\n",
       "      <td>video_data/AT_I2/train_Xiaomi_0108_3241.jpeg</td>\n",
       "      <td>../dados_indoor_location/Xiaomi/Vision/1653920...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>AT_I2</td>\n",
       "      <td>train</td>\n",
       "      <td>109</td>\n",
       "      <td>Redmi</td>\n",
       "      <td>video_data/AT_I2/train_Xiaomi_0109_3271.jpeg</td>\n",
       "      <td>../dados_indoor_location/Xiaomi/Vision/1653920...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3742</th>\n",
       "      <td>AT_I2</td>\n",
       "      <td>train</td>\n",
       "      <td>110</td>\n",
       "      <td>Redmi</td>\n",
       "      <td>video_data/AT_I2/train_Xiaomi_0110_3301.jpeg</td>\n",
       "      <td>../dados_indoor_location/Xiaomi/Vision/1653920...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1684 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label    set  time source                                         image  \\\n",
       "2059     SS  train     4  Redmi     video_data/SS/train_Xiaomi_0004_0121.jpeg   \n",
       "2060     SS  train     5  Redmi     video_data/SS/train_Xiaomi_0005_0151.jpeg   \n",
       "2061     SS  train     6  Redmi     video_data/SS/train_Xiaomi_0006_0181.jpeg   \n",
       "2062     SS  train     7  Redmi     video_data/SS/train_Xiaomi_0007_0211.jpeg   \n",
       "2063     SS  train     8  Redmi     video_data/SS/train_Xiaomi_0008_0241.jpeg   \n",
       "...     ...    ...   ...    ...                                           ...   \n",
       "3738  AT_I2  train   106  Redmi  video_data/AT_I2/train_Xiaomi_0106_3181.jpeg   \n",
       "3739  AT_I2  train   107  Redmi  video_data/AT_I2/train_Xiaomi_0107_3211.jpeg   \n",
       "3740  AT_I2  train   108  Redmi  video_data/AT_I2/train_Xiaomi_0108_3241.jpeg   \n",
       "3741  AT_I2  train   109  Redmi  video_data/AT_I2/train_Xiaomi_0109_3271.jpeg   \n",
       "3742  AT_I2  train   110  Redmi  video_data/AT_I2/train_Xiaomi_0110_3301.jpeg   \n",
       "\n",
       "                                                  video  \n",
       "2059  ../dados_indoor_location/Xiaomi/Vision/1653909...  \n",
       "2060  ../dados_indoor_location/Xiaomi/Vision/1653909...  \n",
       "2061  ../dados_indoor_location/Xiaomi/Vision/1653909...  \n",
       "2062  ../dados_indoor_location/Xiaomi/Vision/1653909...  \n",
       "2063  ../dados_indoor_location/Xiaomi/Vision/1653909...  \n",
       "...                                                 ...  \n",
       "3738  ../dados_indoor_location/Xiaomi/Vision/1653920...  \n",
       "3739  ../dados_indoor_location/Xiaomi/Vision/1653920...  \n",
       "3740  ../dados_indoor_location/Xiaomi/Vision/1653920...  \n",
       "3741  ../dados_indoor_location/Xiaomi/Vision/1653920...  \n",
       "3742  ../dados_indoor_location/Xiaomi/Vision/1653920...  \n",
       "\n",
       "[1684 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('datasets/vision/train.tsv', sep='\\t')\n",
    "SOURCE_FILTER = 'Redmi'\n",
    "if SOURCE_FILTER != None:\n",
    "    train_df = train_df[train_df.source == SOURCE_FILTER]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define image data generator for train dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1684 validated image filenames belonging to 21 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_dataset = datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    target_size=(P_DIM,P_DIM),\n",
    "    batch_size=P_BATCH_SIZE,\n",
    "    class_mode='sparse',\n",
    "    shuffle=True,\n",
    "    seed=1234567,\n",
    "    x_col='image',\n",
    "    y_col='label'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWI8M_iXp-p7"
   },
   "source": [
    "\n",
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6gxhdrHNqO_E",
    "outputId": "dec93355-d63c-48fe-b751-924325fd3da7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Deriving model MobileNetV1\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_2 (KerasLayer)  (None, 1024)              3228864   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 21)                21525     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3250389 (12.40 MB)\n",
      "Trainable params: 21525 (84.08 KB)\n",
      "Non-trainable params: 3228864 (12.32 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "53/53 [==============================] - 16s 268ms/step - loss: 2.3161 - accuracy: 0.3444\n",
      "Epoch 2/25\n",
      "53/53 [==============================] - 14s 253ms/step - loss: 1.1428 - accuracy: 0.6710\n",
      "Epoch 3/25\n",
      "53/53 [==============================] - 13s 244ms/step - loss: 0.8318 - accuracy: 0.7666\n",
      "Epoch 4/25\n",
      "53/53 [==============================] - 13s 242ms/step - loss: 0.6612 - accuracy: 0.8213\n",
      "Epoch 5/25\n",
      "53/53 [==============================] - 13s 245ms/step - loss: 0.5504 - accuracy: 0.8527\n",
      "Epoch 6/25\n",
      "53/53 [==============================] - 13s 250ms/step - loss: 0.4808 - accuracy: 0.8747\n",
      "Epoch 7/25\n",
      "53/53 [==============================] - 13s 244ms/step - loss: 0.4041 - accuracy: 0.8961\n",
      "Epoch 8/25\n",
      "53/53 [==============================] - 13s 245ms/step - loss: 0.3581 - accuracy: 0.9181\n",
      "Epoch 9/25\n",
      "53/53 [==============================] - 14s 252ms/step - loss: 0.3140 - accuracy: 0.9276\n",
      "Epoch 10/25\n",
      "53/53 [==============================] - 13s 250ms/step - loss: 0.2935 - accuracy: 0.9418\n",
      "Epoch 11/25\n",
      "53/53 [==============================] - 13s 243ms/step - loss: 0.2614 - accuracy: 0.9442\n",
      "Epoch 12/25\n",
      "53/53 [==============================] - 13s 242ms/step - loss: 0.2213 - accuracy: 0.9578\n",
      "Epoch 13/25\n",
      "53/53 [==============================] - 13s 249ms/step - loss: 0.2077 - accuracy: 0.9638\n",
      "Epoch 14/25\n",
      "53/53 [==============================] - 13s 250ms/step - loss: 0.1988 - accuracy: 0.9644\n",
      "Epoch 15/25\n",
      "53/53 [==============================] - 13s 245ms/step - loss: 0.1778 - accuracy: 0.9691\n",
      "Epoch 16/25\n",
      "53/53 [==============================] - 14s 254ms/step - loss: 0.1632 - accuracy: 0.9798\n",
      "Epoch 17/25\n",
      "53/53 [==============================] - 14s 253ms/step - loss: 0.1552 - accuracy: 0.9768\n",
      "Epoch 18/25\n",
      "53/53 [==============================] - 13s 250ms/step - loss: 0.1461 - accuracy: 0.9768\n",
      "Epoch 19/25\n",
      "53/53 [==============================] - 14s 256ms/step - loss: 0.1372 - accuracy: 0.9774\n",
      "Epoch 20/25\n",
      "53/53 [==============================] - 14s 257ms/step - loss: 0.1205 - accuracy: 0.9869\n",
      "Epoch 21/25\n",
      "53/53 [==============================] - 14s 257ms/step - loss: 0.1184 - accuracy: 0.9810\n",
      "Epoch 22/25\n",
      "53/53 [==============================] - 14s 254ms/step - loss: 0.1135 - accuracy: 0.9840\n",
      "Epoch 23/25\n",
      "53/53 [==============================] - 13s 247ms/step - loss: 0.1091 - accuracy: 0.9875\n",
      "Epoch 24/25\n",
      "53/53 [==============================] - 14s 257ms/step - loss: 0.0970 - accuracy: 0.9881\n",
      "Epoch 25/25\n",
      "53/53 [==============================] - 14s 253ms/step - loss: 0.0977 - accuracy: 0.9857\n",
      "Training time 337.2013056278229\n",
      "INFO:tensorflow:Assets written to: models/vision/MobileNetV1_Redmi/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/vision/MobileNetV1_Redmi/assets\n",
      "2025-06-18 09:25:20.031476: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-06-18 09:25:20.031506: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-06-18 09:25:20.031717: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: models/vision/MobileNetV1_Redmi\n",
      "2025-06-18 09:25:20.040571: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-06-18 09:25:20.040600: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: models/vision/MobileNetV1_Redmi\n",
      "2025-06-18 09:25:20.065539: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-06-18 09:25:20.257797: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: models/vision/MobileNetV1_Redmi\n",
      "2025-06-18 09:25:20.324784: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 293067 microseconds.\n"
     ]
    }
   ],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global P_CT\n",
    "        if logs.get('accuracy') >= P_CT:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "MODEL_FILTER = None\n",
    "\n",
    "for model_id in MODELS:\n",
    "    if MODEL_FILTER != None and MODEL_FILTER not in model_id:\n",
    "        continue\n",
    "    print('==> Deriving model', model_id)\n",
    "    kl = hub.KerasLayer('models/vision/transfer_learning/' + model_id, trainable=False, input_shape=(P_DIM, P_DIM, 3))\n",
    "    fl = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    if P_DROPOUT > 0.0:\n",
    "        model = tf.keras.Sequential([ kl, tf.keras.layers.Dropout(P_DROPOUT), fl])\n",
    "    else:\n",
    "        model = tf.keras.Sequential([ kl, fl])\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    t_start = time.time()\n",
    "    history = model.fit(train_dataset, epochs=P_MAX_STEPS, callbacks = [CustomCallback()])\n",
    "    t_end = time.time()\n",
    "    print('Training time', t_end - t_start)\n",
    "    if SOURCE_FILTER != None:\n",
    "        model_id = model_id + '_' + SOURCE_FILTER\n",
    "    model.save('models/vision/' + model_id)\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model('models/vision/' + model_id) # path to the SavedModel directory\n",
    "    tflite_model = converter.convert()\n",
    "    with open('models/vision/' + model_id + '/model.tflite', 'wb') as f:\n",
    "      f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mobilenet_model_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
