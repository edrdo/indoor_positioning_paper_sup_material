{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12Y1eMYko_3A"
   },
   "source": [
    "# Vision models - model training using transfer learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vtEv2d4oxJb",
    "outputId": "a7952ffe-ddf1-423f-d484-3247024f1deb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 17:06:02.528770: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-16 17:06:02.572428: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-16 17:06:02.573281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-16 17:06:03.225668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6PLJ6rfpPo-"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IU7U-I9pb8C",
    "outputId": "a69eae70-298a-4b4a-ef88-4502a41800fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e0ccfcbf3141f7b2c02268b8f4efbc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "# MODEL_CHARACTERISTICS\n",
    "MODELS = {\n",
    "    \"InceptionV1\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/inception-v1/TensorFlow2/feature-vector/2\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "    \"InceptionV2\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/inception-v2/TensorFlow2/feature-vector/2\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "    \"MobileNetV1\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/mobilenet-v1/TensorFlow2/100-224-feature-vector/2\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "    \"MobileNetV2\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/mobilenet-v2/TensorFlow2/100-224-feature-vector/2\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "    \"MobileNetV3\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/mobilenet-v3/TensorFlow2/small-100-224-feature-vector/1\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "    \"NasNetMobile\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/nasnet/TensorFlow2/mobile-feature-vector/2\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "    \"ResNetV1\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/resnet-v1/TensorFlow2/50-feature-vector/2\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "    \"ResNetV2\" : {\n",
    "        \"url\": \"https://www.kaggle.com/models/google/resnet-v2/TensorFlow2/50-feature-vector/2\",\n",
    "        \"dim\": 224\n",
    "    },\n",
    "}\n",
    "P_DIM=224\n",
    "NUM_CLASSES=21\n",
    "CT_CFG = widgets.FloatSlider(description=\"Convergenge threshold:\", value=1.0, min=0.25, max=1.00, step=0.01)\n",
    "DROPOUT_CFG = widgets.FloatSlider(description=\"Dropout (0: no dropout):\", value=0.2, min=0.0, max=0.5, step=0.1)\n",
    "MAX_STEPS_CFG = widgets.IntSlider(description=\"Max. training steps:\", value=25, min=1, max=100, step=10)\n",
    "BATCH_SIZE_CFG = widgets.IntSlider(description=\"Batch size:\", value=32, min=32, max=500)\n",
    "display(widgets.VBox([CT_CFG, DROPOUT_CFG, MAX_STEPS_CFG, BATCH_SIZE_CFG]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input dim: 224 \n",
      " Num classes 21 \n",
      " Conv. thr.: 1.0 \n",
      " Dropout: 0.2 \n",
      " Max steps: 25 \n",
      " Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "global P_CT\n",
    "P_CT = CT_CFG.value\n",
    "P_DROPOUT = DROPOUT_CFG.value\n",
    "P_MAX_STEPS = MAX_STEPS_CFG.value\n",
    "P_BATCH_SIZE = BATCH_SIZE_CFG.value\n",
    "print('','Input dim:', P_DIM, '\\n',\n",
    "      'Num classes', NUM_CLASSES, '\\n',\n",
    "      'Conv. thr.:', P_CT, '\\n',\n",
    "      'Dropout:', P_DROPOUT, '\\n',\n",
    "      'Max steps:', P_MAX_STEPS, '\\n',\n",
    "      'Batch size:', P_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>set</th>\n",
       "      <th>time</th>\n",
       "      <th>source</th>\n",
       "      <th>image</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT_CH</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Pixel</td>\n",
       "      <td>video_data/AT_CH/train_Pixel_0000_0000.jpeg</td>\n",
       "      <td>../dados_indoor_location/Pixel/Vision/12182683...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT_CH</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>Pixel</td>\n",
       "      <td>video_data/AT_CH/train_Pixel_0001_0031.jpeg</td>\n",
       "      <td>../dados_indoor_location/Pixel/Vision/12182683...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT_CH</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "      <td>Pixel</td>\n",
       "      <td>video_data/AT_CH/train_Pixel_0003_0091.jpeg</td>\n",
       "      <td>../dados_indoor_location/Pixel/Vision/12182683...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AT_CH</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>Pixel</td>\n",
       "      <td>video_data/AT_CH/train_Pixel_0004_0121.jpeg</td>\n",
       "      <td>../dados_indoor_location/Pixel/Vision/12182683...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AT_CH</td>\n",
       "      <td>train</td>\n",
       "      <td>5</td>\n",
       "      <td>Pixel</td>\n",
       "      <td>video_data/AT_CH/train_Pixel_0005_0151.jpeg</td>\n",
       "      <td>../dados_indoor_location/Pixel/Vision/12182683...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>AT_I2</td>\n",
       "      <td>train</td>\n",
       "      <td>106</td>\n",
       "      <td>Xiaomi</td>\n",
       "      <td>video_data/AT_I2/train_Xiaomi_0106_3181.jpeg</td>\n",
       "      <td>../dados_indoor_location/Xiaomi/Vision/1653920...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>AT_I2</td>\n",
       "      <td>train</td>\n",
       "      <td>107</td>\n",
       "      <td>Xiaomi</td>\n",
       "      <td>video_data/AT_I2/train_Xiaomi_0107_3211.jpeg</td>\n",
       "      <td>../dados_indoor_location/Xiaomi/Vision/1653920...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>AT_I2</td>\n",
       "      <td>train</td>\n",
       "      <td>108</td>\n",
       "      <td>Xiaomi</td>\n",
       "      <td>video_data/AT_I2/train_Xiaomi_0108_3241.jpeg</td>\n",
       "      <td>../dados_indoor_location/Xiaomi/Vision/1653920...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>AT_I2</td>\n",
       "      <td>train</td>\n",
       "      <td>109</td>\n",
       "      <td>Xiaomi</td>\n",
       "      <td>video_data/AT_I2/train_Xiaomi_0109_3271.jpeg</td>\n",
       "      <td>../dados_indoor_location/Xiaomi/Vision/1653920...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3742</th>\n",
       "      <td>AT_I2</td>\n",
       "      <td>train</td>\n",
       "      <td>110</td>\n",
       "      <td>Xiaomi</td>\n",
       "      <td>video_data/AT_I2/train_Xiaomi_0110_3301.jpeg</td>\n",
       "      <td>../dados_indoor_location/Xiaomi/Vision/1653920...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3743 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label    set  time  source  \\\n",
       "0     AT_CH  train     0   Pixel   \n",
       "1     AT_CH  train     1   Pixel   \n",
       "2     AT_CH  train     3   Pixel   \n",
       "3     AT_CH  train     4   Pixel   \n",
       "4     AT_CH  train     5   Pixel   \n",
       "...     ...    ...   ...     ...   \n",
       "3738  AT_I2  train   106  Xiaomi   \n",
       "3739  AT_I2  train   107  Xiaomi   \n",
       "3740  AT_I2  train   108  Xiaomi   \n",
       "3741  AT_I2  train   109  Xiaomi   \n",
       "3742  AT_I2  train   110  Xiaomi   \n",
       "\n",
       "                                             image  \\\n",
       "0      video_data/AT_CH/train_Pixel_0000_0000.jpeg   \n",
       "1      video_data/AT_CH/train_Pixel_0001_0031.jpeg   \n",
       "2      video_data/AT_CH/train_Pixel_0003_0091.jpeg   \n",
       "3      video_data/AT_CH/train_Pixel_0004_0121.jpeg   \n",
       "4      video_data/AT_CH/train_Pixel_0005_0151.jpeg   \n",
       "...                                            ...   \n",
       "3738  video_data/AT_I2/train_Xiaomi_0106_3181.jpeg   \n",
       "3739  video_data/AT_I2/train_Xiaomi_0107_3211.jpeg   \n",
       "3740  video_data/AT_I2/train_Xiaomi_0108_3241.jpeg   \n",
       "3741  video_data/AT_I2/train_Xiaomi_0109_3271.jpeg   \n",
       "3742  video_data/AT_I2/train_Xiaomi_0110_3301.jpeg   \n",
       "\n",
       "                                                  video  \n",
       "0     ../dados_indoor_location/Pixel/Vision/12182683...  \n",
       "1     ../dados_indoor_location/Pixel/Vision/12182683...  \n",
       "2     ../dados_indoor_location/Pixel/Vision/12182683...  \n",
       "3     ../dados_indoor_location/Pixel/Vision/12182683...  \n",
       "4     ../dados_indoor_location/Pixel/Vision/12182683...  \n",
       "...                                                 ...  \n",
       "3738  ../dados_indoor_location/Xiaomi/Vision/1653920...  \n",
       "3739  ../dados_indoor_location/Xiaomi/Vision/1653920...  \n",
       "3740  ../dados_indoor_location/Xiaomi/Vision/1653920...  \n",
       "3741  ../dados_indoor_location/Xiaomi/Vision/1653920...  \n",
       "3742  ../dados_indoor_location/Xiaomi/Vision/1653920...  \n",
       "\n",
       "[3743 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('datasets/vision/train.tsv', sep='\\t')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define image data generator for train dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3743 validated image filenames belonging to 21 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_dataset = datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    target_size=(P_DIM,P_DIM),\n",
    "    batch_size=P_BATCH_SIZE,\n",
    "    class_mode='sparse',\n",
    "    shuffle=True,\n",
    "    seed=1234567,\n",
    "    x_col='image',\n",
    "    y_col='label'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWI8M_iXp-p7"
   },
   "source": [
    "\n",
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6gxhdrHNqO_E",
    "outputId": "dec93355-d63c-48fe-b751-924325fd3da7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Deriving model InceptionV1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_1 (KerasLayer)  (None, 1024)              5607184   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 21)                21525     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5628709 (21.47 MB)\n",
      "Trainable params: 21525 (84.08 KB)\n",
      "Non-trainable params: 5607184 (21.39 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "117/117 [==============================] - 42s 339ms/step - loss: 1.9153 - accuracy: 0.4956\n",
      "Epoch 2/25\n",
      "117/117 [==============================] - 40s 341ms/step - loss: 1.0971 - accuracy: 0.7232\n",
      "Epoch 3/25\n",
      "117/117 [==============================] - 40s 343ms/step - loss: 0.8715 - accuracy: 0.7769\n",
      "Epoch 4/25\n",
      "117/117 [==============================] - 40s 343ms/step - loss: 0.7478 - accuracy: 0.8036\n",
      "Epoch 5/25\n",
      "117/117 [==============================] - 41s 345ms/step - loss: 0.6645 - accuracy: 0.8274\n",
      "Epoch 6/25\n",
      "117/117 [==============================] - 40s 344ms/step - loss: 0.6045 - accuracy: 0.8432\n",
      "Epoch 7/25\n",
      "117/117 [==============================] - 41s 346ms/step - loss: 0.5639 - accuracy: 0.8493\n",
      "Epoch 8/25\n",
      "117/117 [==============================] - 41s 346ms/step - loss: 0.5182 - accuracy: 0.8627\n",
      "Epoch 9/25\n",
      "117/117 [==============================] - 41s 345ms/step - loss: 0.4935 - accuracy: 0.8664\n",
      "Epoch 10/25\n",
      "117/117 [==============================] - 41s 349ms/step - loss: 0.4599 - accuracy: 0.8822\n",
      "Epoch 11/25\n",
      "117/117 [==============================] - 41s 350ms/step - loss: 0.4342 - accuracy: 0.8918\n",
      "Epoch 12/25\n",
      "117/117 [==============================] - 41s 350ms/step - loss: 0.4190 - accuracy: 0.8889\n",
      "Epoch 13/25\n",
      "117/117 [==============================] - 40s 343ms/step - loss: 0.3942 - accuracy: 0.8950\n",
      "Epoch 14/25\n",
      "117/117 [==============================] - 41s 345ms/step - loss: 0.3886 - accuracy: 0.8998\n",
      "Epoch 15/25\n",
      "117/117 [==============================] - 41s 347ms/step - loss: 0.3685 - accuracy: 0.9014\n",
      "Epoch 16/25\n",
      "117/117 [==============================] - 41s 348ms/step - loss: 0.3468 - accuracy: 0.9113\n",
      "Epoch 17/25\n",
      "117/117 [==============================] - 41s 350ms/step - loss: 0.3407 - accuracy: 0.9158\n",
      "Epoch 18/25\n",
      "117/117 [==============================] - 41s 350ms/step - loss: 0.3283 - accuracy: 0.9126\n",
      "Epoch 19/25\n",
      "117/117 [==============================] - 41s 350ms/step - loss: 0.3178 - accuracy: 0.9174\n",
      "Epoch 20/25\n",
      "117/117 [==============================] - 41s 353ms/step - loss: 0.3052 - accuracy: 0.9233\n",
      "Epoch 21/25\n",
      "117/117 [==============================] - 41s 352ms/step - loss: 0.2874 - accuracy: 0.9241\n",
      "Epoch 22/25\n",
      "117/117 [==============================] - 41s 351ms/step - loss: 0.2832 - accuracy: 0.9263\n",
      "Epoch 23/25\n",
      "117/117 [==============================] - 41s 351ms/step - loss: 0.2733 - accuracy: 0.9311\n",
      "Epoch 24/25\n",
      "117/117 [==============================] - 41s 350ms/step - loss: 0.2714 - accuracy: 0.9300\n",
      "Epoch 25/25\n",
      "117/117 [==============================] - 41s 351ms/step - loss: 0.2665 - accuracy: 0.9308\n",
      "Training time 1104.4808864593506\n",
      "INFO:tensorflow:Assets written to: models/vision/InceptionV1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/vision/InceptionV1/assets\n",
      "2025-05-16 17:26:52.000091: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-05-16 17:26:52.000120: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-05-16 17:26:52.000445: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: models/vision/InceptionV1\n",
      "2025-05-16 17:26:52.011424: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-05-16 17:26:52.011457: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: models/vision/InceptionV1\n",
      "2025-05-16 17:26:52.047968: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-05-16 17:26:52.057766: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-05-16 17:26:52.388950: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: models/vision/InceptionV1\n",
      "2025-05-16 17:26:52.502954: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 502510 microseconds.\n",
      "2025-05-16 17:26:52.630294: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Deriving model InceptionV2\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_2 (KerasLayer)  (None, 1024)              10173112  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 21)                21525     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10194637 (38.89 MB)\n",
      "Trainable params: 21525 (84.08 KB)\n",
      "Non-trainable params: 10173112 (38.81 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "117/117 [==============================] - 53s 434ms/step - loss: 2.0183 - accuracy: 0.4544\n",
      "Epoch 2/25\n",
      "117/117 [==============================] - 52s 441ms/step - loss: 1.1318 - accuracy: 0.7128\n",
      "Epoch 3/25\n",
      "117/117 [==============================] - 52s 443ms/step - loss: 0.8987 - accuracy: 0.7694\n",
      "Epoch 4/25\n",
      "117/117 [==============================] - 52s 445ms/step - loss: 0.7686 - accuracy: 0.7962\n",
      "Epoch 5/25\n",
      "117/117 [==============================] - 52s 444ms/step - loss: 0.6896 - accuracy: 0.8202\n",
      "Epoch 6/25\n",
      "117/117 [==============================] - 52s 446ms/step - loss: 0.6266 - accuracy: 0.8298\n",
      "Epoch 7/25\n",
      "117/117 [==============================] - 52s 445ms/step - loss: 0.5746 - accuracy: 0.8453\n",
      "Epoch 8/25\n",
      "117/117 [==============================] - 52s 445ms/step - loss: 0.5339 - accuracy: 0.8595\n",
      "Epoch 9/25\n",
      "117/117 [==============================] - 52s 447ms/step - loss: 0.5010 - accuracy: 0.8592\n",
      "Epoch 10/25\n",
      "117/117 [==============================] - 52s 446ms/step - loss: 0.4843 - accuracy: 0.8734\n",
      "Epoch 11/25\n",
      "117/117 [==============================] - 52s 446ms/step - loss: 0.4570 - accuracy: 0.8782\n",
      "Epoch 12/25\n",
      "117/117 [==============================] - 52s 446ms/step - loss: 0.4374 - accuracy: 0.8832\n",
      "Epoch 13/25\n",
      "117/117 [==============================] - 53s 448ms/step - loss: 0.4139 - accuracy: 0.8910\n",
      "Epoch 14/25\n",
      "117/117 [==============================] - 53s 451ms/step - loss: 0.3968 - accuracy: 0.8958\n",
      "Epoch 15/25\n",
      "117/117 [==============================] - 52s 446ms/step - loss: 0.3793 - accuracy: 0.8985\n",
      "Epoch 16/25\n",
      "117/117 [==============================] - 53s 448ms/step - loss: 0.3685 - accuracy: 0.9022\n",
      "Epoch 17/25\n",
      "117/117 [==============================] - 52s 446ms/step - loss: 0.3524 - accuracy: 0.8990\n",
      "Epoch 18/25\n",
      "117/117 [==============================] - 52s 447ms/step - loss: 0.3382 - accuracy: 0.9097\n",
      "Epoch 19/25\n",
      "117/117 [==============================] - 53s 448ms/step - loss: 0.3263 - accuracy: 0.9102\n",
      "Epoch 20/25\n",
      "117/117 [==============================] - 52s 446ms/step - loss: 0.3166 - accuracy: 0.9132\n",
      "Epoch 21/25\n",
      "117/117 [==============================] - 53s 448ms/step - loss: 0.3131 - accuracy: 0.9169\n",
      "Epoch 22/25\n",
      "117/117 [==============================] - 53s 449ms/step - loss: 0.2986 - accuracy: 0.9228\n",
      "Epoch 23/25\n",
      "117/117 [==============================] - 53s 448ms/step - loss: 0.2969 - accuracy: 0.9212\n",
      "Epoch 24/25\n",
      "117/117 [==============================] - 53s 448ms/step - loss: 0.2814 - accuracy: 0.9276\n",
      "Epoch 25/25\n",
      "117/117 [==============================] - 53s 448ms/step - loss: 0.2785 - accuracy: 0.9265\n",
      "Training time 1311.7127785682678\n",
      "INFO:tensorflow:Assets written to: models/vision/InceptionV2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/vision/InceptionV2/assets\n",
      "2025-05-16 17:48:52.846725: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-05-16 17:48:52.846756: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-05-16 17:48:52.846976: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: models/vision/InceptionV2\n",
      "2025-05-16 17:48:52.868310: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-05-16 17:48:52.868345: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: models/vision/InceptionV2\n",
      "2025-05-16 17:48:52.922601: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-05-16 17:48:53.319174: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: models/vision/InceptionV2\n",
      "2025-05-16 17:48:53.458041: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 611065 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Deriving model MobileNetV1\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_3 (KerasLayer)  (None, 1024)              3228864   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 21)                21525     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3250389 (12.40 MB)\n",
      "Trainable params: 21525 (84.08 KB)\n",
      "Non-trainable params: 3228864 (12.32 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "117/117 [==============================] - 31s 251ms/step - loss: 1.7237 - accuracy: 0.5170\n",
      "Epoch 2/25\n",
      "117/117 [==============================] - 31s 261ms/step - loss: 0.8015 - accuracy: 0.7785\n",
      "Epoch 3/25\n",
      "117/117 [==============================] - 30s 257ms/step - loss: 0.5960 - accuracy: 0.8336\n",
      "Epoch 4/25\n",
      "117/117 [==============================] - 31s 260ms/step - loss: 0.4803 - accuracy: 0.8707\n",
      "Epoch 5/25\n",
      "117/117 [==============================] - 30s 258ms/step - loss: 0.4100 - accuracy: 0.8894\n",
      "Epoch 6/25\n",
      "117/117 [==============================] - 31s 262ms/step - loss: 0.3566 - accuracy: 0.9041\n",
      "Epoch 7/25\n",
      "117/117 [==============================] - 31s 260ms/step - loss: 0.3070 - accuracy: 0.9217\n",
      "Epoch 8/25\n",
      "117/117 [==============================] - 31s 261ms/step - loss: 0.2801 - accuracy: 0.9311\n",
      "Epoch 9/25\n",
      "117/117 [==============================] - 29s 248ms/step - loss: 0.2481 - accuracy: 0.9391\n",
      "Epoch 10/25\n",
      "117/117 [==============================] - 30s 251ms/step - loss: 0.2321 - accuracy: 0.9410\n",
      "Epoch 11/25\n",
      "117/117 [==============================] - 31s 265ms/step - loss: 0.2152 - accuracy: 0.9468\n",
      "Epoch 12/25\n",
      "117/117 [==============================] - 30s 256ms/step - loss: 0.1901 - accuracy: 0.9559\n",
      "Epoch 13/25\n",
      "117/117 [==============================] - 29s 250ms/step - loss: 0.1814 - accuracy: 0.9578\n",
      "Epoch 14/25\n",
      "117/117 [==============================] - 30s 256ms/step - loss: 0.1636 - accuracy: 0.9631\n",
      "Epoch 15/25\n",
      "117/117 [==============================] - 31s 261ms/step - loss: 0.1577 - accuracy: 0.9610\n",
      "Epoch 16/25\n",
      "117/117 [==============================] - 31s 260ms/step - loss: 0.1520 - accuracy: 0.9647\n",
      "Epoch 17/25\n",
      "117/117 [==============================] - 31s 260ms/step - loss: 0.1408 - accuracy: 0.9671\n",
      "Epoch 18/25\n",
      "117/117 [==============================] - 31s 264ms/step - loss: 0.1286 - accuracy: 0.9711\n",
      "Epoch 19/25\n",
      "117/117 [==============================] - 31s 262ms/step - loss: 0.1229 - accuracy: 0.9717\n",
      "Epoch 20/25\n",
      "117/117 [==============================] - 30s 252ms/step - loss: 0.1151 - accuracy: 0.9760\n",
      "Epoch 21/25\n",
      "117/117 [==============================] - 29s 247ms/step - loss: 0.1057 - accuracy: 0.9760\n",
      "Epoch 22/25\n",
      "117/117 [==============================] - 30s 259ms/step - loss: 0.1069 - accuracy: 0.9770\n",
      "Epoch 23/25\n",
      "117/117 [==============================] - 30s 256ms/step - loss: 0.1052 - accuracy: 0.9752\n",
      "Epoch 24/25\n",
      "117/117 [==============================] - 31s 262ms/step - loss: 0.0951 - accuracy: 0.9808\n",
      "Epoch 25/25\n",
      "117/117 [==============================] - 31s 263ms/step - loss: 0.0934 - accuracy: 0.9792\n",
      "Training time 769.5095715522766\n",
      "INFO:tensorflow:Assets written to: models/vision/MobileNetV1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/vision/MobileNetV1/assets\n",
      "2025-05-16 18:01:48.446287: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-05-16 18:01:48.446314: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-05-16 18:01:48.446538: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: models/vision/MobileNetV1\n",
      "2025-05-16 18:01:48.454197: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-05-16 18:01:48.454225: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: models/vision/MobileNetV1\n",
      "2025-05-16 18:01:48.475749: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-05-16 18:01:48.673732: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: models/vision/MobileNetV1\n",
      "2025-05-16 18:01:48.745772: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 299235 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Deriving model MobileNetV2\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_4 (KerasLayer)  (None, 1280)              2257984   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 21)                26901     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2284885 (8.72 MB)\n",
      "Trainable params: 26901 (105.08 KB)\n",
      "Non-trainable params: 2257984 (8.61 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "117/117 [==============================] - 34s 270ms/step - loss: 1.6267 - accuracy: 0.5464\n",
      "Epoch 2/25\n",
      "117/117 [==============================] - 31s 265ms/step - loss: 0.7906 - accuracy: 0.7809\n",
      "Epoch 3/25\n",
      "117/117 [==============================] - 32s 270ms/step - loss: 0.6041 - accuracy: 0.8320\n",
      "Epoch 4/25\n",
      "117/117 [==============================] - 29s 249ms/step - loss: 0.4992 - accuracy: 0.8670\n",
      "Epoch 5/25\n",
      "117/117 [==============================] - 31s 268ms/step - loss: 0.4299 - accuracy: 0.8841\n",
      "Epoch 6/25\n",
      "117/117 [==============================] - 32s 269ms/step - loss: 0.3695 - accuracy: 0.9057\n",
      "Epoch 7/25\n",
      "117/117 [==============================] - 30s 257ms/step - loss: 0.3248 - accuracy: 0.9209\n",
      "Epoch 8/25\n",
      "117/117 [==============================] - 30s 258ms/step - loss: 0.2932 - accuracy: 0.9289\n",
      "Epoch 9/25\n",
      "117/117 [==============================] - 32s 270ms/step - loss: 0.2584 - accuracy: 0.9388\n",
      "Epoch 10/25\n",
      "117/117 [==============================] - 30s 253ms/step - loss: 0.2419 - accuracy: 0.9410\n",
      "Epoch 11/25\n",
      "117/117 [==============================] - 31s 267ms/step - loss: 0.2178 - accuracy: 0.9492\n",
      "Epoch 12/25\n",
      "117/117 [==============================] - 31s 264ms/step - loss: 0.2051 - accuracy: 0.9538\n",
      "Epoch 13/25\n",
      "117/117 [==============================] - 31s 266ms/step - loss: 0.1923 - accuracy: 0.9516\n",
      "Epoch 14/25\n",
      "117/117 [==============================] - 30s 252ms/step - loss: 0.1756 - accuracy: 0.9594\n",
      "Epoch 15/25\n",
      "117/117 [==============================] - 30s 257ms/step - loss: 0.1616 - accuracy: 0.9631\n",
      "Epoch 16/25\n",
      "117/117 [==============================] - 30s 254ms/step - loss: 0.1548 - accuracy: 0.9637\n",
      "Epoch 17/25\n",
      "117/117 [==============================] - 30s 259ms/step - loss: 0.1442 - accuracy: 0.9674\n",
      "Epoch 18/25\n",
      "117/117 [==============================] - 30s 253ms/step - loss: 0.1339 - accuracy: 0.9725\n",
      "Epoch 19/25\n",
      "117/117 [==============================] - 30s 252ms/step - loss: 0.1248 - accuracy: 0.9746\n",
      "Epoch 20/25\n",
      "117/117 [==============================] - 30s 253ms/step - loss: 0.1246 - accuracy: 0.9727\n",
      "Epoch 21/25\n",
      "117/117 [==============================] - 32s 268ms/step - loss: 0.1134 - accuracy: 0.9757\n",
      "Epoch 22/25\n",
      "117/117 [==============================] - 31s 267ms/step - loss: 0.1112 - accuracy: 0.9800\n",
      "Epoch 23/25\n",
      "117/117 [==============================] - 31s 260ms/step - loss: 0.1078 - accuracy: 0.9770\n",
      "Epoch 24/25\n",
      "117/117 [==============================] - 32s 269ms/step - loss: 0.0960 - accuracy: 0.9824\n",
      "Epoch 25/25\n",
      "117/117 [==============================] - 31s 267ms/step - loss: 0.0994 - accuracy: 0.9792\n",
      "Training time 771.3827986717224\n",
      "INFO:tensorflow:Assets written to: models/vision/MobileNetV2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/vision/MobileNetV2/assets\n",
      "2025-05-16 18:14:47.761033: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-05-16 18:14:47.761065: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-05-16 18:14:47.761283: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: models/vision/MobileNetV2\n",
      "2025-05-16 18:14:47.780236: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-05-16 18:14:47.780270: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: models/vision/MobileNetV2\n",
      "2025-05-16 18:14:47.827830: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-05-16 18:14:48.173570: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: models/vision/MobileNetV2\n",
      "2025-05-16 18:14:48.298813: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 537531 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Deriving model MobileNetV3\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_5 (KerasLayer)  (None, 1024)              1529968   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 21)                21525     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1551493 (5.92 MB)\n",
      "Trainable params: 21525 (84.08 KB)\n",
      "Non-trainable params: 1529968 (5.84 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "117/117 [==============================] - 25s 193ms/step - loss: 1.6217 - accuracy: 0.5707\n",
      "Epoch 2/25\n",
      "117/117 [==============================] - 21s 178ms/step - loss: 0.7852 - accuracy: 0.8050\n",
      "Epoch 3/25\n",
      "117/117 [==============================] - 21s 176ms/step - loss: 0.5963 - accuracy: 0.8469\n",
      "Epoch 4/25\n",
      "117/117 [==============================] - 23s 194ms/step - loss: 0.4871 - accuracy: 0.8771\n",
      "Epoch 5/25\n",
      "117/117 [==============================] - 21s 177ms/step - loss: 0.4204 - accuracy: 0.8923\n",
      "Epoch 6/25\n",
      "117/117 [==============================] - 23s 193ms/step - loss: 0.3683 - accuracy: 0.9044\n",
      "Epoch 7/25\n",
      "117/117 [==============================] - 22s 183ms/step - loss: 0.3231 - accuracy: 0.9207\n",
      "Epoch 8/25\n",
      "117/117 [==============================] - 22s 184ms/step - loss: 0.2940 - accuracy: 0.9321\n",
      "Epoch 9/25\n",
      "117/117 [==============================] - 22s 190ms/step - loss: 0.2692 - accuracy: 0.9372\n",
      "Epoch 10/25\n",
      "117/117 [==============================] - 21s 177ms/step - loss: 0.2457 - accuracy: 0.9402\n",
      "Epoch 11/25\n",
      "117/117 [==============================] - 22s 191ms/step - loss: 0.2248 - accuracy: 0.9492\n",
      "Epoch 12/25\n",
      "117/117 [==============================] - 22s 188ms/step - loss: 0.2113 - accuracy: 0.9522\n",
      "Epoch 13/25\n",
      "117/117 [==============================] - 22s 184ms/step - loss: 0.1970 - accuracy: 0.9578\n",
      "Epoch 14/25\n",
      "117/117 [==============================] - 21s 175ms/step - loss: 0.1832 - accuracy: 0.9583\n",
      "Epoch 15/25\n",
      "117/117 [==============================] - 23s 193ms/step - loss: 0.1732 - accuracy: 0.9631\n",
      "Epoch 16/25\n",
      "117/117 [==============================] - 22s 188ms/step - loss: 0.1634 - accuracy: 0.9669\n",
      "Epoch 17/25\n",
      "117/117 [==============================] - 22s 185ms/step - loss: 0.1490 - accuracy: 0.9711\n",
      "Epoch 18/25\n",
      "117/117 [==============================] - 21s 183ms/step - loss: 0.1470 - accuracy: 0.9695\n",
      "Epoch 19/25\n",
      "117/117 [==============================] - 21s 176ms/step - loss: 0.1379 - accuracy: 0.9725\n",
      "Epoch 20/25\n",
      "117/117 [==============================] - 21s 177ms/step - loss: 0.1244 - accuracy: 0.9792\n",
      "Epoch 21/25\n",
      "117/117 [==============================] - 21s 176ms/step - loss: 0.1263 - accuracy: 0.9770\n",
      "Epoch 22/25\n",
      "117/117 [==============================] - 21s 177ms/step - loss: 0.1201 - accuracy: 0.9768\n",
      "Epoch 23/25\n",
      "117/117 [==============================] - 21s 178ms/step - loss: 0.1149 - accuracy: 0.9778\n",
      "Epoch 24/25\n",
      "117/117 [==============================] - 21s 175ms/step - loss: 0.1087 - accuracy: 0.9792\n",
      "Epoch 25/25\n",
      "117/117 [==============================] - 22s 191ms/step - loss: 0.1040 - accuracy: 0.9816\n",
      "Training time 540.2837698459625\n",
      "INFO:tensorflow:Assets written to: models/vision/MobileNetV3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/vision/MobileNetV3/assets\n",
      "2025-05-16 18:23:55.553734: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-05-16 18:23:55.553764: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-05-16 18:23:55.553969: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: models/vision/MobileNetV3\n",
      "2025-05-16 18:23:55.569645: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-05-16 18:23:55.569679: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: models/vision/MobileNetV3\n",
      "2025-05-16 18:23:55.611766: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-05-16 18:23:55.902688: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: models/vision/MobileNetV3\n",
      "2025-05-16 18:23:56.012134: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 458165 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Deriving model NasNetMobile\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_6 (KerasLayer)  (None, 1056)              4269716   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1056)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 21)                22197     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4291913 (16.37 MB)\n",
      "Trainable params: 22197 (86.71 KB)\n",
      "Non-trainable params: 4269716 (16.29 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "117/117 [==============================] - 49s 337ms/step - loss: 1.9439 - accuracy: 0.4625\n",
      "Epoch 2/25\n",
      "117/117 [==============================] - 40s 343ms/step - loss: 1.1141 - accuracy: 0.6968\n",
      "Epoch 3/25\n",
      "117/117 [==============================] - 40s 340ms/step - loss: 0.8872 - accuracy: 0.7612\n",
      "Epoch 4/25\n",
      "117/117 [==============================] - 40s 336ms/step - loss: 0.7724 - accuracy: 0.7881\n",
      "Epoch 5/25\n",
      "117/117 [==============================] - 40s 338ms/step - loss: 0.6880 - accuracy: 0.8116\n",
      "Epoch 6/25\n",
      "117/117 [==============================] - 40s 342ms/step - loss: 0.6313 - accuracy: 0.8271\n",
      "Epoch 7/25\n",
      "117/117 [==============================] - 40s 341ms/step - loss: 0.5815 - accuracy: 0.8333\n",
      "Epoch 8/25\n",
      "117/117 [==============================] - 40s 338ms/step - loss: 0.5425 - accuracy: 0.8496\n",
      "Epoch 9/25\n",
      "117/117 [==============================] - 40s 341ms/step - loss: 0.5126 - accuracy: 0.8587\n",
      "Epoch 10/25\n",
      "117/117 [==============================] - 40s 341ms/step - loss: 0.4703 - accuracy: 0.8750\n",
      "Epoch 11/25\n",
      "117/117 [==============================] - 40s 342ms/step - loss: 0.4628 - accuracy: 0.8694\n",
      "Epoch 12/25\n",
      "117/117 [==============================] - 40s 338ms/step - loss: 0.4409 - accuracy: 0.8734\n",
      "Epoch 13/25\n",
      "117/117 [==============================] - 41s 347ms/step - loss: 0.4117 - accuracy: 0.8859\n",
      "Epoch 14/25\n",
      "117/117 [==============================] - 40s 341ms/step - loss: 0.3970 - accuracy: 0.8929\n",
      "Epoch 15/25\n",
      "117/117 [==============================] - 40s 343ms/step - loss: 0.3812 - accuracy: 0.8974\n",
      "Epoch 16/25\n",
      "117/117 [==============================] - 40s 341ms/step - loss: 0.3613 - accuracy: 0.9057\n",
      "Epoch 17/25\n",
      "117/117 [==============================] - 41s 346ms/step - loss: 0.3504 - accuracy: 0.9089\n",
      "Epoch 18/25\n",
      "117/117 [==============================] - 40s 339ms/step - loss: 0.3464 - accuracy: 0.9078\n",
      "Epoch 19/25\n",
      "117/117 [==============================] - 40s 340ms/step - loss: 0.3287 - accuracy: 0.9121\n",
      "Epoch 20/25\n",
      "117/117 [==============================] - 41s 347ms/step - loss: 0.3202 - accuracy: 0.9172\n",
      "Epoch 21/25\n",
      "117/117 [==============================] - 41s 351ms/step - loss: 0.3087 - accuracy: 0.9204\n",
      "Epoch 22/25\n",
      "117/117 [==============================] - 40s 342ms/step - loss: 0.2988 - accuracy: 0.9217\n",
      "Epoch 23/25\n",
      "117/117 [==============================] - 40s 341ms/step - loss: 0.2918 - accuracy: 0.9209\n",
      "Epoch 24/25\n",
      "117/117 [==============================] - 40s 342ms/step - loss: 0.2836 - accuracy: 0.9279\n",
      "Epoch 25/25\n",
      "117/117 [==============================] - 41s 346ms/step - loss: 0.2750 - accuracy: 0.9303\n",
      "Training time 1015.964189529419\n",
      "INFO:tensorflow:Assets written to: models/vision/NasNetMobile/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/vision/NasNetMobile/assets\n",
      "2025-05-16 18:41:20.401616: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-05-16 18:41:20.401643: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-05-16 18:41:20.401870: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: models/vision/NasNetMobile\n",
      "2025-05-16 18:41:20.479569: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-05-16 18:41:20.479598: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: models/vision/NasNetMobile\n",
      "2025-05-16 18:41:20.691960: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-05-16 18:41:22.332584: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: models/vision/NasNetMobile\n",
      "2025-05-16 18:41:22.898235: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 2496366 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Deriving model ResNetV1\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_7 (KerasLayer)  (None, 2048)              23561152  \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 21)                43029     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23604181 (90.04 MB)\n",
      "Trainable params: 43029 (168.08 KB)\n",
      "Non-trainable params: 23561152 (89.88 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "117/117 [==============================] - 98s 820ms/step - loss: 1.3307 - accuracy: 0.6196\n",
      "Epoch 2/25\n",
      "117/117 [==============================] - 97s 827ms/step - loss: 0.6117 - accuracy: 0.8221\n",
      "Epoch 3/25\n",
      "117/117 [==============================] - 97s 828ms/step - loss: 0.4322 - accuracy: 0.8792\n",
      "Epoch 4/25\n",
      "117/117 [==============================] - 97s 828ms/step - loss: 0.3356 - accuracy: 0.9113\n",
      "Epoch 5/25\n",
      "117/117 [==============================] - 124s 1s/step - loss: 0.2716 - accuracy: 0.9311\n",
      "Epoch 6/25\n",
      "117/117 [==============================] - 128s 1s/step - loss: 0.2230 - accuracy: 0.9458\n",
      "Epoch 7/25\n",
      "117/117 [==============================] - 107s 913ms/step - loss: 0.1916 - accuracy: 0.9554\n",
      "Epoch 8/25\n",
      "117/117 [==============================] - 97s 830ms/step - loss: 0.1659 - accuracy: 0.9629\n",
      "Epoch 9/25\n",
      "117/117 [==============================] - 97s 827ms/step - loss: 0.1463 - accuracy: 0.9677\n",
      "Epoch 10/25\n",
      "117/117 [==============================] - 97s 828ms/step - loss: 0.1288 - accuracy: 0.9746\n",
      "Epoch 11/25\n",
      "117/117 [==============================] - 97s 827ms/step - loss: 0.1158 - accuracy: 0.9778\n",
      "Epoch 12/25\n",
      "117/117 [==============================] - 97s 829ms/step - loss: 0.1046 - accuracy: 0.9842\n",
      "Epoch 13/25\n",
      "117/117 [==============================] - 97s 828ms/step - loss: 0.0918 - accuracy: 0.9837\n",
      "Epoch 14/25\n",
      "117/117 [==============================] - 97s 830ms/step - loss: 0.0814 - accuracy: 0.9885\n",
      "Epoch 15/25\n",
      "117/117 [==============================] - 96s 815ms/step - loss: 0.0749 - accuracy: 0.9893\n",
      "Epoch 16/25\n",
      "117/117 [==============================] - 95s 810ms/step - loss: 0.0702 - accuracy: 0.9917\n",
      "Epoch 17/25\n",
      "117/117 [==============================] - 95s 810ms/step - loss: 0.0668 - accuracy: 0.9915\n",
      "Epoch 18/25\n",
      "117/117 [==============================] - 96s 820ms/step - loss: 0.0596 - accuracy: 0.9920\n",
      "Epoch 19/25\n",
      "117/117 [==============================] - 96s 821ms/step - loss: 0.0596 - accuracy: 0.9909\n",
      "Epoch 20/25\n",
      "117/117 [==============================] - 96s 823ms/step - loss: 0.0517 - accuracy: 0.9941\n",
      "Epoch 21/25\n",
      "117/117 [==============================] - 97s 824ms/step - loss: 0.0463 - accuracy: 0.9949\n",
      "Epoch 22/25\n",
      "117/117 [==============================] - 95s 809ms/step - loss: 0.0434 - accuracy: 0.9952\n",
      "Epoch 23/25\n",
      "117/117 [==============================] - 96s 819ms/step - loss: 0.0428 - accuracy: 0.9957\n",
      "Epoch 24/25\n",
      "117/117 [==============================] - 96s 822ms/step - loss: 0.0384 - accuracy: 0.9960\n",
      "Epoch 25/25\n",
      "117/117 [==============================] - 96s 823ms/step - loss: 0.0377 - accuracy: 0.9963\n",
      "Training time 2684.5442185401917\n",
      "INFO:tensorflow:Assets written to: models/vision/ResNetV1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/vision/ResNetV1/assets\n",
      "2025-05-16 19:26:18.008035: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-05-16 19:26:18.008073: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-05-16 19:26:18.008279: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: models/vision/ResNetV1\n",
      "2025-05-16 19:26:18.026678: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-05-16 19:26:18.026709: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: models/vision/ResNetV1\n",
      "2025-05-16 19:26:18.076354: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-05-16 19:26:18.480486: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: models/vision/ResNetV1\n",
      "2025-05-16 19:26:18.608821: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 600542 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Deriving model ResNetV2\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_8 (KerasLayer)  (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 21)                43029     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23607829 (90.06 MB)\n",
      "Trainable params: 43029 (168.08 KB)\n",
      "Non-trainable params: 23564800 (89.89 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "117/117 [==============================] - 110s 916ms/step - loss: 1.4716 - accuracy: 0.5830\n",
      "Epoch 2/25\n",
      "117/117 [==============================] - 108s 922ms/step - loss: 0.6874 - accuracy: 0.8050\n",
      "Epoch 3/25\n",
      "117/117 [==============================] - 108s 921ms/step - loss: 0.5016 - accuracy: 0.8653\n",
      "Epoch 4/25\n",
      "117/117 [==============================] - 108s 923ms/step - loss: 0.3999 - accuracy: 0.8977\n",
      "Epoch 5/25\n",
      "117/117 [==============================] - 108s 923ms/step - loss: 0.3295 - accuracy: 0.9223\n",
      "Epoch 6/25\n",
      "117/117 [==============================] - 108s 924ms/step - loss: 0.2831 - accuracy: 0.9311\n",
      "Epoch 7/25\n",
      "117/117 [==============================] - 108s 925ms/step - loss: 0.2476 - accuracy: 0.9428\n",
      "Epoch 8/25\n",
      "117/117 [==============================] - 108s 924ms/step - loss: 0.2175 - accuracy: 0.9503\n",
      "Epoch 9/25\n",
      "117/117 [==============================] - 108s 925ms/step - loss: 0.1874 - accuracy: 0.9626\n",
      "Epoch 10/25\n",
      "117/117 [==============================] - 108s 925ms/step - loss: 0.1657 - accuracy: 0.9671\n",
      "Epoch 11/25\n",
      "117/117 [==============================] - 110s 938ms/step - loss: 0.1496 - accuracy: 0.9725\n",
      "Epoch 12/25\n",
      "117/117 [==============================] - 109s 926ms/step - loss: 0.1352 - accuracy: 0.9746\n",
      "Epoch 13/25\n",
      "117/117 [==============================] - 108s 922ms/step - loss: 0.1284 - accuracy: 0.9754\n",
      "Epoch 14/25\n",
      "117/117 [==============================] - 108s 923ms/step - loss: 0.1156 - accuracy: 0.9818\n",
      "Epoch 15/25\n",
      "117/117 [==============================] - 108s 921ms/step - loss: 0.1078 - accuracy: 0.9853\n",
      "Epoch 16/25\n",
      "117/117 [==============================] - 108s 921ms/step - loss: 0.0969 - accuracy: 0.9890\n",
      "Epoch 17/25\n",
      "117/117 [==============================] - 108s 920ms/step - loss: 0.0932 - accuracy: 0.9866\n",
      "Epoch 18/25\n",
      "117/117 [==============================] - 108s 920ms/step - loss: 0.0825 - accuracy: 0.9882\n",
      "Epoch 19/25\n",
      "117/117 [==============================] - 108s 921ms/step - loss: 0.0788 - accuracy: 0.9893\n",
      "Epoch 20/25\n",
      "117/117 [==============================] - 108s 922ms/step - loss: 0.0731 - accuracy: 0.9909\n",
      "Epoch 21/25\n",
      "117/117 [==============================] - 108s 919ms/step - loss: 0.0707 - accuracy: 0.9893\n",
      "Epoch 22/25\n",
      "117/117 [==============================] - 107s 909ms/step - loss: 0.0667 - accuracy: 0.9909\n",
      "Epoch 23/25\n",
      "117/117 [==============================] - 108s 925ms/step - loss: 0.0596 - accuracy: 0.9933\n",
      "Epoch 24/25\n",
      "117/117 [==============================] - 107s 916ms/step - loss: 0.0570 - accuracy: 0.9936\n",
      "Epoch 25/25\n",
      "117/117 [==============================] - 107s 916ms/step - loss: 0.0529 - accuracy: 0.9931\n",
      "Training time 2773.4287934303284\n",
      "INFO:tensorflow:Assets written to: models/vision/ResNetV2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/vision/ResNetV2/assets\n",
      "2025-05-16 20:12:43.564840: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-05-16 20:12:43.564871: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-05-16 20:12:43.565099: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: models/vision/ResNetV2\n",
      "2025-05-16 20:12:43.579062: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-05-16 20:12:43.579096: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: models/vision/ResNetV2\n",
      "2025-05-16 20:12:43.632263: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-05-16 20:12:44.117864: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: models/vision/ResNetV2\n",
      "2025-05-16 20:12:44.273317: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 708219 microseconds.\n"
     ]
    }
   ],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global P_CT\n",
    "        if logs.get('accuracy') >= P_CT:\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "for model_id in MODELS:\n",
    "    print('==> Deriving model', model_id)\n",
    "    kl = hub.KerasLayer('models/vision/transfer_learning/' + model_id, trainable=False, input_shape=(P_DIM, P_DIM, 3))\n",
    "    fl = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "\n",
    "    if P_DROPOUT > 0.0:\n",
    "        model = tf.keras.Sequential([ kl, tf.keras.layers.Dropout(P_DROPOUT), fl])\n",
    "    else:\n",
    "        model = tf.keras.Sequential([ kl, fl])\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    t_start = time.time()\n",
    "    history = model.fit(train_dataset, epochs=P_MAX_STEPS, callbacks = [CustomCallback()])\n",
    "    t_end = time.time()\n",
    "    print('Training time', t_end - t_start)\n",
    "    model.save('models/vision/' + model_id)\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model('models/vision/' + model_id) # path to the SavedModel directory\n",
    "    tflite_model = converter.convert()\n",
    "    with open('models/vision/' + model_id + '/model.tflite', 'wb') as f:\n",
    "      f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oxFM-9qwcg9"
   },
   "source": [
    "## Save model and convert it to TFLite format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_map = (train_dataset.class_indices)\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mobilenet_model_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
